This file describes how to download and preprocess a Wikipedia dump into
a TextGrounder corpus.

=========== Quick start ==========
Use the following to download a Wikipedia dump and preprocess it to create
a corpus for input to TextGrounder:

$ download-preprocess-wiki WIKITAG

where WIKITAG is something like 'dewiki-20120225', which names an existing
version of Wikipedia on http://dumps.wikipedia.org.  This downloads the
given Wikipedia dump into a subdirectory of the current directory
named WIKITAG, and then preprocesses it into a corpus of the format
needed by TextGrounder.

$ download-preprocess-wiki 'preprocess-dump' to run all of the steps below.
This calls 'run-processwiki'.

=========== Introduction ==========
There are a large number of steps that go into preprocessing a Wikipedia
dump, and a number of different files containing various sorts of
information.  Not all of them actually go into the final corpus normally
used by TextGrounder.  Some of the complexity of this process is due to
the fact that it was created bit-by-bit and evolved over time at the same
time that TextGrounder itself did.

The preprocessing process involves downloading the dump file from
http://dumps.wikipedia.org (which has a name like
enwiki-20120211-pages-articles.xml.bz2), and going through a series of
steps that involve generating various files, terminating with files in the
standard TextGrounder corpus format, which will have names similar to the
following:

1. enwiki-20120211-permuted-dev-unigram-counts.txt (the actual data for the
   documents in the dev set)
2. enwiki-20120211-permuted-dev-unigram-counts-schema.txt (a small "schema"
   file describing the fields contained in the data file, plus certain other
   information)
3. a similar pair of files for the training and test sets.

Note that part of the proess involves permuting (i.e. randomly reordering)
the articles in the dump file, leading to the creation of a new permuted
dump file with a name like enwiki-20120211-permuted-pages-articles.xml.bz2.
(The order of articles in the raw, unpermuted dump file is highly non-random.)
This is done so that the training, dev and test sets can be extracted in
a simple fashion and so that partial results can be reported during the
testing phase, with an expectation that they will reliably reflect the
final results.


From highest to lowest, there are the following levels of scripts:

1. 'download-preprocess-wiki' does everything from start to finish,
   downloading a dump, preprocessing it into various output files, and
   using them to generate a TextGrounder corpus.  It uses 'wget' to
   download the dump; 'preprocess-dump' to process the dump and generate
   data files sufficient to generate a corpus; and 'convert-corpus-to-latest'
   to actually generate the corpus.

2. 'preprocess-dump' takes a raw dump and processes it in various ways in
   order to generate a number of output files.  For historical reasons,
   a TextGrounder corpus isn't directly generated but rather an older
   data format containing equivalent information.  An additional script
   ('convert-corpus-to-latest') is needed to convert this older format
   to the TextGrounder corpus format.  In addition, some extra data files
   are generated that aren't currently used by TextGrounder. (Eventually,
   these issues should be fixed.) 'preprocess-dump' uses some lower-level
   scripts to generate the data files.  Most important among them is
   'run-processwiki'.

3. 'run-processwiki' is a driver script that can be run in various modes
   to generate various sorts of preprocessed data files from the raw
   Wikipedia dump.  It runs the actual preprocessing script (processwiki.py)
   with the appropriate arguments, providing the dump file as standard input.

4. 'processwiki.py' is the actual script that creates the data files.  It
   takes command-line arguments specifying modes to run in and files
   containing ancillary information of various sorts, including information
   previously generated the same script when run in a different mode.  The
   dump file is provided as the standard input of this script rather than
   through a command-line argument, and generates its data files on its
   standard output.  'processwiki.py' has no concept of any naming convention
   for the files it generates; it is the job of 'run-processwiki' to keep
   track of the various files.


For 'run-processwiki' and 'preprocess-dump', the dump file used for input
and the various generated files are stored into and read from the current
directory.  'download-preprocess-wiki' creates a subdirectory of the
current directory with the name of the dump being processed (e.g.
'enwiki-20120317').  

When directly running any script below the top-level script
'download-preprocess-wiki', it's strongly suggested that

1) The dump file be made read-only using 'chmod a-w'.
2) A separate directory is created to hold the various generated preprocessed
   data files.  Either the dump file should be moved into this directory
   or a symlink to the dump file created in the directory.


Note also the file 'config-geolocate'.  This is normally sourced into the
various scripts, and sets a number of environment variables, including those
naming the dump file and the various intermediate data files produced.
This relies on the following environment variables:

WP_VERSION       Specifies which dump file to use, e.g. "enwiki-20111007".
USE_PERMUTED     If set to "no", uses the non-permuted version of the dump
                 file.  If set to "yes", always try to use the permuted
                 version.  If blank or unset, use permuted version if it
                 appears to exist, non-permuted otherwise.

These two variables can be set on the command line used to execute
'run-processwiki'.


============== How preprocessing works, in detail ===============

The following describes in detail the set of steps executed by
'download-preprocess-wiki' to generate a corpus.  It shows how individual
steps could be rerun if needed.  We assume that we are working with the
dump named "enwiki-20111007".

1. A new directory 'enwiki-20111007' is created to hold the downloaded dump
   file and all the generated files.  The dump is downloaded into this
   directory as follows:

wget -nd http://dumps.wikimedia.org/enwiki/20111007/enwiki-20111007-pages-articles.xml.bz2

   In all future steps, we work with this new directory as the current
   directory.

2. The basic article data file 'enwiki-20111007-document-data.txt' is
   created, giving metadata on the articles in the raw dump file
   'enwiki-20111007-pages-articles.xml.bz2', one article per line:

USE_PERMUTED=no run-processwiki article-data

   Note that, in this and future invocations of 'run-processwiki', we set
   the environment variable WP_VERSION to 'enwiki-20111007' to tell it
   the prefix of the dump file and the generated output files.

3. A permuted dump file 'enwiki-20111007-permuted-pages-articles.xml.bz2' is
   generated.  This uses the article data file from the previous step to
   specify the list of articles.  Basically, this list is permuted randomly,
   then divided into 8 parts according to the new order.  Then, the dump file
   is processed serially article-by-article, outputting the article into one
   of the 8 split files, according to the part it belongs in.  Then, each
   split file is sorted in memory and written out again, and finally the
   parts are concatenated and compressed.  This avoids having to read the
   entire dump file into memory and sort it, which may not be possible.

run-permute all

4. Split the permuted dump into 8 parts, for faster processing.  This could
   probably re-use the parts from the previous step, but currently it is
   written to be an independent operation.

run-processwiki split-dump

   Note also that from here on out, we set USE_PERMUTED to 'yes' to make
   sure that we use the permuted dump file. ('run-processwiki' attempts to
   auto-detect whether to use the permuted file if USE_PERMUTED isn't set,
   but it's safer to specify this explicitly.)

5. Generate a combined article data file from the permuted dump file.

run-processwiki combined-article-data

Note that from here on out, we set NUM_SIMULTANEOUS to 8 to indicate that
we should do parallel operation on the 8 split parts.  This will spawn off
8 processes to handle the separate parts, except in certain cases where
the parts cannot be processed separately.

This step actually has a number of subparts:

   a. Generate 'enwiki-20111007-permuted-article-data.txt', a basic article
      data file for the permuted dump.  This is like the article data file
      generated above for the raw, non-permuted dump.  It has one line of
      metadata for each article, with a simple database-like format with
      fields separated by tab characters, where the first line specifies the
      name of each field.

   b. Generate 'enwiki-20111007-permuted-coords.txt', specifying the
      coordinates (geotags) of articles in the dump file, when such
      information can be extracted.  Extracting the information involves a
      large amount of tricky and tedious parsing of the articles.

   c. Generate 'enwiki-20111007-permuted-links-only-coord-documents.txt',
      listing, for all articles with coordinates, the count of incoming
      internal links pointing to them from some other article.  Also contains,
      for each such article,  a list of all the anchor texts used when pointing
      to that article, along with associated counts. (This additional
      information is not currently used.)

   d. Generate 'enwiki-20111007-permuted-combined-document-data.txt'.
      This combines the information from the previous three steps.  This
      file has the same format as the basic article data file, but has two
      additional fields, specifying the coordinates and incoming link count.
      Unlike the basic article data file, it only lists articles with
      coordinates.

6. Generate 'enwiki-20111007-permuted-counts-only-coord-documents.txt',
   the word-counts file for articles with coordinates.  This is the remaining
   info needed for creating a TextGrounder corpus.

run-processwiki coord-counts

7. Generate some additional files, not currently needed:

   a. 'enwiki-20111007-permuted-counts-all-documents.txt': Word counts for
      all articles, not just those with coordinates.

   b. 'enwiki-20111007-permuted-text-only-coord-documents.txt': The actual
      text, pre-tokenized into words, of articles with coordinates.

   c. 'enwiki-20111007-permuted-text-all-documents.txt': The actual text,
      pre-tokenized into words, of all articles.

8. Remove the various intermediate split files created by the above steps.

9. Generate the corpus files.  This involves combining the metadata in
   'enwiki-20111007-permuted-combined-document-data.txt' with the word-count
   information in 'enwiki-20111007-permuted-counts-only-coord-documents.txt'
   into a combined file with one line per document; splitting this data into
   three files, one each for the training, dev and test sets; and creating
   corresponding schema files.

   This is done as follows:

convert-corpus-to-latest enwiki-20111007

============== How to rerun a single step ===============

If all the preprocessing has already been done for you, and you simply want
to run a single step, then you don't need to do all of the above steps.
However, it's still strongly recommended that you do your work in a fresh
directory, and symlink the dump file into that directory -- in this case the
*permuted* dump file.  We use the permuted dump file for experiments because
the raw dump file has a non-uniform distribution of articles, and so we can't
e.g. count on our splits being uniformly distributed.  Randomly permuting
the dump file and article lists takes care of that.  The permuted dump file
has a name like

enwiki-20111007-permuted-pages-articles.xml.bz2

For example, if want to change processwiki.py to generate bigrams, and then
run it to generate the bigram counts, you might do this:

1. Note that there are currently options `output-coord-counts` to output
   unigram counts only for articles with coordinates (which are the only ones
   needed for standard document geotagging), and `output-all-counts` to
   output unigram counts for all articles.  You want to add corresponding
   options for bigram counts -- either something like
   `output-coord-bigram-counts` and `output-all-bigram-counts`, or an option
   `--n-gram` to specify the N-gram size (1 for unigrams, 2 for bigrams,
   3 for trigrams if that's implemented, etc.).  *DO NOT* in any circumstance
   simply hack the code so that it automatically outputs bigrams instead of
   unigrams -- such code CANNOT be incorporated into the repository, which
   means your mods will become orphaned and unavailable for anyone else.

2. Modify 'config-geolocate' so that it has additional sets of environment
   variables for bigram counts.  For example, after these lines:

COORD_COUNTS_SUFFIX="counts-only-coord-documents.txt"
ALL_COUNTS_SUFFIX="counts-all-documents.txt"

   you'd add

COORD_BIGRAM_COUNTS_SUFFIX="bigram-counts-only-coord-documents.txt"
ALL_BIGRAM_COUNTS_SUFFIX="bigram-counts-all-documents.txt"

   Similarly, after these lines:

OUT_COORD_COUNTS_FILE="$DUMP_PREFIX-$COORD_COUNTS_SUFFIX"
OUT_ALL_COUNTS_FILE="$DUMP_PREFIX-$ALL_COUNTS_SUFFIX"

   you'd add

OUT_COORD_BIGRAM_COUNTS_FILE="$DUMP_PREFIX-$COORD_BIGRAM_COUNTS_SUFFIX"
OUT_ALL_BIGRAM_COUNTS_FILE="$DUMP_PREFIX-$ALL_BIGRAM_COUNTS_SUFFIX"

   And then you'd do the same thing for IN_COORD_COUNTS_FILE and
   IN_ALL_COUNTS_FILE.

3. Modify 'run-processwiki', adding new targets ("steps")
   'coord-bigram-counts' and 'all-bigram-counts'.  Here, you would just
   copy the existing lines for 'coord-counts' and 'all-counts' and modify
   them appropriately.

4. Now finally you can run it:

WP_VERSION=enwiki-20111007 run-processwiki coord-bigram-counts

   This generates the bigram counts for geotagged articles -- the minimum
   necessary for document geotagging.

   Actually, since the above might take awhile and generate a fair amount
   of diagnostic input, you might want to run it in the background
   under nohup, so that it won't die if your terminal connection suddenly
   dies.  One way to do that is to use the TextGrounder 'run-nohup' script:

WP_VERSION=enwiki-20111007 run-nohup --id do-coord-bigram-counts run-processwiki coord-bigram-counts

   Note that the '--id do-coord-bigram-counts' is optional; all it does is
   insert the text "do-coord-bigram-counts" into the file that it stores
   stdout and stderr output into.  This file will have a name beginning
   'run-nohup.' and ending with a timestamp.  The beginning and ending of the
   file will indicate the starting and ending times, so you can see how long
   it took.

   If you want to generate bigram counts for all articles, you could use a
   similar command line, although it might take a couple of days to complete.
   If you're on Longhorn, where you only have 24-hour time slots, you might
   consider using the "divide-and-conquer" mode.  The first thing is to
   split the dump file, like this:

WP_VERSION=enwiki-20111007 run-processwiki split-dump

   This takes maybe 45 mins and splits the whole dump file into 8 pieces.
   (Controllable through NUM_SPLITS.)

   Then, each operation you want to do in divide-and-conquer mode, run it
   by setting NUM_SIMULTANEOUS to something more than 1, e.g.

WP_VERSION=enwiki-20111007 NUM_SIMULTANEOUS=8 run-processwiki all-bigram-counts

   (although you probably want to wrap it in 'run-nohup').  Essentially,
   this runs 8 simultaneous run-processwiki processes (which fits well with
   the workhorse Longhorn machines, since they are 8-core), one on each of
   the 8 splits, and then concatenates the results together at the end.
   You can set a NUM_SIMULTANEOUS that's lower than the number of splits,
   and you get only that much simultaneity.

============== Inpu files, intermediate files, output files ===============

These are specified from the point of view of a dump named "enwiki-20111007",
in approximate order of generation.

enwiki-20111007-pages-articles.xml.bz2
  Raw dump of article text, downloaded from
  http://download.wikimedia.org/enwiki/20111007/


, specifying coordinates (geotags) of articles in the dump file,
  when such information can be extracted.  Extracting the information
  involves a large amount of tricky and tedious parsing of the articles.

enwiki-20111007-coords-counts.txt
  -- Output from passing raw dump through processwiki.py --coords-counts
  -- For all articles with attached coordinates, lists the article name,
     coordinates, and counts of all words seen as part of the "useful text"
     (filtering out directives and such; see processwiki.py for more info)

enwiki-20111007-just-titles-coords.txt
  -- Result of filtering output of --coords-counts for only the article
     title and coordinates, with word counts omitted.

enwiki-20111007-links-only-coord-articles.txt
  -- Result of running the following:

     processwiki.py --find-links \
       --coords-file enwiki-20111007-just-titles-coords.out

  -- For each article with coordinates, lists the number of incoming links
     to that article.  In addition, a set of tables listing surface forms of
     links and the corresponding articles linked to, with counts, is output.
     The surface form of a link is the text that the user sees.  Basically,
     the entire set of links pointing to articles with coordinates is grouped
     according to the surface form of the link, and for each different surface
     form, a table is output listing all articles linked to and the
     corresponding counts.  This data might be useful in constructing a
     disambiguator for mapping surface forms to canonical forms for geographic
     entities (i.e. mapping toponyms to locations).


Other data that could be useful:

-- Map of all redirects (e.g. "U.S." -> United States)
-- Map of all disambiguation pages and the articles linked to
-- The inverse surface->article mapping, i.e. for each article, list of
   surface forms that link to the article, along with counts
-- List of all pairs of articles that mutually link to each other, along with
   counts: Useful for indicating closely linked articles
-- Tables of categories (constructed from articles with a [[Category:foo]]
   link in them) and lists (construct from "List of foo" articles)
